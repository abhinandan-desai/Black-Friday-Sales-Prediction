---
title: "Trim Your Sales!"
author: "Abhinandan Desai, Sarthak Gupte"
date: "6/9/2020"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
library(tidyverse)
library(scales)
library(arules)
library(gridExtra)
library(dplyr)
library(glmnet)
library(randomForest)
library(ranger)
library(caret)
library(doParallel)
library(coefplot)
library(xgboost)
```

```{r wrap-hook, echo=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```


# **Problem Description**

The project title is a word play on the idiom 'Trim your sails'. The idiom means to make changes to suit one's new circumstances. In business, it generally means to change the management strategy, reduce costs due to changes and unforeseen circumstances. This is a major problem in retail stores and this project aims to create a solution to predict future sales for companies so that it can enable a better strategic plan beforehand, reduce risks and handle emergency situations.  

# **Introduction**

The process of estimating future sales is called sales forecasting. Accurate sales forecasts can enable companies to make strategic business decisions and predict the performance of sales in the near future. It also gives an insight to work out the workforce, costs, and resources of a company. Past sales data, economic, seasonal and location trends are some of the major factors that help to generate accurate forecasts. Predictive sales also help companies and businesses to plan their future growth and expansion.

This project is going to focus on creating a system that will conduct regression analysis on sales data of Black Friday and try to predict accurate future sales. The dataset contains different features which will be used to create a more detailed analysis.

The project will firstly collect data, then it will focus on cleaning and preparing the data. The data will be then explored and visualized for better understanding. The project will use different regression techniques to generate machine learning models in order to carry out prediction of sales. One of the following algorithms will be worked upon for improvement to generate more accurate predictions. A comparitive analysis will be carried out between the algorithms. The algorithms that are being considered are as follows:

- Ridge Regression - It helps to mitigate the problem of multicollinearity which is common in models with many parameters. It improves parameter estimation within considerable bias.

- Lasso Regression - It is the abbreviation of **L**east **A**bsolute **S**hrinkage and **S**election **O**perator. It  performs both variable selection and regularization in order to enhance the prediction accuracy.

- Elastic Net Regression - Elastic net is a combination of ridge and lasso regression. It has two tuning parameters (alpha and lambda) while lasso and ridge regression only has 1.

- Random Forest Regression - It operates by constructing a number of decision trees parallely at training time and outputting the mean prediction of the individual trees.

- XGBoost (e**X**treme **G**radient **B**oosting) Regression - It is an advanced implementation of gradient boosting algorithm. It can handle sparse data. 

Such a system is a necessity nowadays in any kind of business. Large amounts of data is generated by retail stores and Walmart is one such example. The data would go to waste if not used properly and this project will try to make the most of such data. It will help find general and hidden sale trends which will further lead to better strategic plans.

# **Data Summary and Visualization**

The dataset contains 12 different columns, each attribute representing a different characteristic of the dataset:

-User_ID: Unique identifier of the shopper.

-Product_ID: Unique identifier for the purchased product. 

-Gender: Identification of the shopper (Male/Female).

-Age: Shopper's age that has been divided into bins or groups.

-Occupation: Occupation of the shopper. It is categorical as each number must be linked to a type of occupation

-City_Category: City where the shopper lives.

-Stay_In_Current_City_Years: Number of years that the shopper has stayed in current city.

-Marital_Status: Whether the shopper is married or not. value '0' corresponding to unmarried and '1' corresponding to married.

-Product_Category_1: Product category of the purchase.

-Product_Category_2: Product category of the purchase.

-Product_Category_3: Product category of the purchase.

-Purchase: Value of the product in American dollars.

The structure of the dataset is as follows and is then followed by an example of the actual dataset:
```{r dataCSV, linewidth=80,echo=FALSE}
bf.data = read.csv("BlackFriday.csv", stringsAsFactors = TRUE )
bf.data[is.na(bf.data)] = 0 #replacing N/A values with 0
str(bf.data)
# summary(bf.data)
head(bf.data)
```
Each row of the dataset represents a different transaction or item purchased by a specific customer. The Product_ID identifies the product that has been purchased and the User_ID identifies the customer that purchased it.

In order to predict sales, the target attribute is 'Purchase' in the dataset. A correlation plot would help to identify attributes that have any relation or affect that attribute.

```{r correlation, linewidth = 90, echo=FALSE}
bf.sample = bf.data[sample(nrow(bf.data), 1000), ]
par(mar=c(1,1,1,1))
pairs(bf.sample, cex=0.1)
```

Different attributes are analyzed in order to understand the distribution of sales in the dataset. Firstly, the 'Gender' attribute is analyzed in order to understand the distribution of sales among males and females 

```{r genderDist, echo=FALSE}
bf.gender = bf.data %>%
                    select(User_ID, Gender) %>%
                    group_by(User_ID) %>%
                    distinct()  
summary(bf.gender$Gender)
x = summary(bf.gender$Gender)
g.labels = c("Females", "Males")
pct = round(x/sum(x)*100)
g.labels = paste(g.labels, "(") 
g.labels = paste(g.labels, pct) # add percents to labels
g.labels = paste(g.labels,"%)",sep="") # ad % to labels 
genderDist = pie(x, labels=g.labels, main = "Gender Distribution of Shoppers")
```

It is visible that majority of the shoppers are Males. This is a good insight which could help understand the stores what kind of products have better chances to be sold but in order to understand it better, the average spending by each gender is analyzed to get a better insight of amount spend in sales.

```{r userIDAndGender, echo=FALSE}
bf.userID = bf.data %>%
                    select(User_ID, Gender, Purchase) %>%
                    group_by(User_ID) %>%
                    arrange(User_ID) %>%
                    summarize(Total_Purchase = sum(Purchase))

bf.userGender = bf.data %>%
                        select(User_ID, Gender) %>%
                        group_by(User_ID) %>%
                        arrange(User_ID) %>%
                        distinct()
                
head(bf.userID)
head(bf.userGender)
```

The total amount of dollars each shopper has spent is now known and there is also a dataset that knows the gender of each shopper. Merging these datasets into one and then calculating the average spendings that each gender group has done on Black Friday is the next step.

```{r avgGender, echo=FALSE}
bf.userID.Gender = full_join(bf.userID, bf.userGender, by = "User_ID")
head(bf.userID.Gender)
bf.avgGender = bf.userID.Gender %>%
                                group_by(Gender) %>%
                                summarize(Purchase = sum(as.numeric(Total_Purchase)), 
                                          Count = n(), 
                                          Average = Purchase/Count)
head(bf.avgGender)
genderAverage  = ggplot(data = bf.avgGender) +
                    geom_bar(mapping = aes(x = Gender, y = Average, fill = Gender), stat = 'identity') +
                    labs(title = 'Average Spending by Gender')
print(genderAverage)
```

The average spending by males on Black Friday is considerably higher than the females. Further, checking the distribution of the buyers or shoppers on the basis of the marital status can give insights on the kind of products that might be more popular and whether or not being married has a major effect on spending.

```{r maritalStatus, echo=FALSE}
bf.maritalStatus =  bf.data %>%
                            select(User_ID, Marital_Status) %>%
                            group_by(User_ID) %>%
                            distinct()
                            
head(bf.maritalStatus)

bf.maritalStatus$Marital_Status = as.character(bf.maritalStatus$Marital_Status)

married = 0
unmarried = 0

for(value in bf.maritalStatus$Marital_Status){
  if(value == 0){
    unmarried = unmarried + 1
  } else {1
  married = married + 1
  }
}


maritalData = data.frame(
    Status = factor(c("Married","Unmarried"), levels=c("Married","Unmarried")),
    Count = c(married, unmarried)
)

maritalDist = ggplot(data=maritalData, aes(x=Status, y=Count, fill=Status)) +
    geom_bar(stat="identity") + labs(title = " Marital Status of Shoppers")

print(maritalDist)
```

Most shoppers seem to be unmarried. Similarly, visualizing the distribution of 'Age' atrribute in the dataset would help to find out which age group is dominant as shoppers.

```{r ageDist, echo=FALSE}
bf.age = bf.data %>%
                 select(User_ID, Age) %>%
                 distinct() %>%
                 count(Age)
print(bf.age)
x = bf.age$n
age.lbls = c("0-17", "18-25","26-35", "36-45","46-50", "51-55", "55+")
pct = round(x/sum(x)*100)
age.lbls =  paste(age.lbls, "(") 
age.lbls =paste(age.lbls, pct) # add percents to labels
age.lbls = paste(age.lbls,"%)",sep="") # add % to labels 
pie(x, labels=age.lbls, main = "Age Group Distribution of Shoppers")
```

From the pie chart it can be seen that the Age group of 26-35 have a majority in number of sales happening on black friday. After this, to increase purchases and sales and to understand the trends, finding out the top products that sell become helpful to understand the trends.

```{r topSell, echo = FALSE}
bf.topSell = bf.data %>%
                count(Product_ID, sort = TRUE)

top5 = head(bf.topSell, 5)

top5
```

P00265242 is the top seller in the store and it is clearly sold more than any other products. Analyzing the major buyers of this product will help to understand the type of product it is.

```{r topSellPlot, echo=FALSE}
bf.bestSeller = bf.data[bf.data$Product_ID == 'P00265242', ] # all entries of top selling product

bf.topSell.genderDist  = ggplot(data = bf.bestSeller) +
                  geom_bar(mapping = aes(x = Gender, y = ..count.., fill = Gender)) +
                  labs(title = 'Best Seller Sales by Gender') 
print(bf.topSell.genderDist)
```

The best selling product is mostly bought by males. These visualizations and analysis of the dataset helped to understand about the products, the trends in sales, and also the type of customers in the store. It can be deduced that most spending customers are unmarried males and are in the age group of 26-35. The average spending by males is comparatively higher than females. The best selling product is mostly bought by male customers.


# **Data Preprocessing**
It is necessary to clean and prepare data so that it is best suited for further analysis. In order to achieve that, columns 'User_ID' and 'Product_ID' that don't add information for analysis are dropped.

```{R dropColumns, echo=FALSE}
blackFriday = read.csv("BlackFriday.csv", stringsAsFactors = TRUE )
blackFriday[is.na(blackFriday)] = 0
blackFriday.analysis = select(blackFriday, -c("User_ID","Product_ID"))
head(blackFriday.analysis)
```

After this we divide the data into train and test data. 70% data as train data and 30% data as test data. The 'Purchase' attribute is the target variable.

```{r trainTest, echo=FALSE}
set.seed(100) 

index = sample(1:nrow(blackFriday.analysis), 0.7*nrow(blackFriday.analysis)) 

train = blackFriday.analysis[index,] # Create the training data 
test = blackFriday.analysis[-index,] # Create the test data

dim(train)
dim(test)

y.train = train$Purchase
x.train = select(train, -c("Purchase")) %>% data.matrix()

y.test = test$Purchase
x.test = select(test, -c("Purchase")) %>% data.matrix()
```

Once the required data is prepared, regression analysis can be carried out in order to predict the variables that affect sales the most.

# **Ridge Regression**

Regression is an extension of a linear regression where the loss function is modified. This minimize the complexity using alpha. Alpha value is set to zero which represents Ridge Regression. 
Firstly, ridge regression involves tuning a hyperparameter, lambda. An argument for a range of lambdas is created. Cross validation is carried out to find the best lambda value. 

```{r ridgeRegression,, linewidth = 90, echo=FALSE}

lambdas = 10^seq(3, -2, by = -.1)

fit.ridge = glmnet(x.train, y.train, alpha = 0, lambda = lambdas, standardize = TRUE)
summary(fit.ridge)

cv.fit = cv.glmnet(x.train, y.train, alpha = 0, lambda = lambdas, standardize = TRUE)
plot(cv.fit)
optimum.lambda = cv.fit$lambda.min

y.predicted = predict(fit.ridge, s = optimum.lambda, newx = x.train)

# Sum of Squares Total and Error for Training Data
sst = sum((y.train - mean(y.train))^2)
sse = sum((y.predicted - y.train)^2)

# R squared
rsq = 1 - sse / sst
rsq

# RMSE
rmse = sqrt(sse/nrow(train))
rmse


y.predicted.test = predict(fit.ridge, s = optimum.lambda, newx = x.test)

# Sum of Squares Total and Error for Testing Data
sst = sum((y.test - mean(y.test))^2)
sse = sum((y.predicted.test - y.test)^2)

# R squared
rsq = 1 - sse / sst
cat("Rsquared Value for Test data = ", rsq, "\n")

# RMSE
rmse.ridge = sqrt(sse/nrow(train))
cat("RMSE Value for Test data = ", rmse.ridge)

plot(fit.ridge, xvar = "lambda", label=T, xlim = c(-4,8))
abline(v=cv.fit$lambda.min, col = "red", lty=2)
abline(v=cv.fit$lambda.1se, col="blue", lty=2)   #Higher lambda value for more regularizartion
title("Ridge Regression", line=2.5)
```

Two lambda values are selected for the model. The Coefficient VS Log Lambda contains a red dotted line which represents the lambda minimum that results in the smallest cross-validation error. This value is generally used as the penalization level for predicting outcomes in the dataset. 'lambda$1se' is the lambda at one standard error and is represented by a blue dotted line. It is used for higher penalization level for predicting outcomes in the dataset. 

The Log Lambda value of 'lambda$1se' is around 100, hence cannot be visualized in the graph. In the graph plotted, neither of the attributes show a lot of variation. At Lambda minimum, we can say that attribute 1,4,7 and 9 could be good predictors. 


# **Lasso Regression**

After analyzing results of Ridge regression, data is analyzed using LASSO Regression. Lasso is also modified version of linear regression. Here, alpha value becomes 1 which represents Lasso regression. Firstly, the optimum lambda is found.

```{r lassoRegression, , linewidth = 90, echo=FALSE}
lambdas = 10^seq(2, -3, by = -.1)
fit.lasso = glmnet(x.train, y.train, alpha = 1, lambda = lambdas, standardize = TRUE)

cv.fit = cv.glmnet(x.train, y.train, alpha = 1, lambda = lambdas, standardize = TRUE)
plot(cv.fit)
optimum.lambda = cv.fit$lambda.min 



lasso.y.predicted = predict(fit.lasso, s = optimum.lambda, newx = x.train)

# Sum of Squares Total and Error for Training Data
sst = sum((y.train - mean(y.train))^2)
sse = sum((lasso.y.predicted - y.train)^2)

# R squared
rsq = 1 - sse / sst
rsq

# RMSE
rmse = sqrt(sse/nrow(train))
rmse


lasso.y.predicted.test = predict(fit.lasso, s = optimum.lambda, newx = x.test)

# Sum of Squares Total and Error for Testing Data
sst = sum((y.test - mean(y.test))^2)
sse = sum((lasso.y.predicted.test - y.test)^2)

# R squared
rsq = 1 - sse / sst
cat("Rsquared Value for Test data = ", rsq, "\n")

# RMSE
rmse.lasso = sqrt(sse/nrow(train))
cat("RMSE Value for Test data = ", rmse.lasso)

plot(fit.lasso, xvar = "lambda", label=T, asp=0.01)
abline(v=cv.fit$lambda.min, col = "red", lty=2)
abline(v=cv.fit$lambda.1se, col="blue", lty=2)
title("Lasso Regression", line=2.5)
```

In the Coefficient VS Log Lambda plot, it can be observed that variation in the predictors is more. We can observe that just after lambda minimum the variation of the attributes 1, 4 and 7 is high. Lasso Regression uses a different penalization approach and so some coefficients become exactly zero. The attrbutes 1,4 and 7 are visibly good predictors for sales. 6 has a small coefficient but it also has a good variation.

# **Elastic Net Regression (Core Algorithm)**

Elastic Net Regression has alpha between 0 and 1. It uses a combination of both penalization approaches from Lasso and Ridge regression techniques. 

```{r elasticNetRegression,, linewidth = 90, echo=FALSE}
lambdas = 10^seq(2, -3, by = -.1)
fit.enet = glmnet(x.train, y.train, alpha = 0.5, lambda = lambdas, standardize = TRUE)

cv.fit = cv.glmnet(x.train, y.train, alpha = 0.5, lambda = lambdas, standardize = TRUE)
plot(cv.fit)
optimum.lambda = cv.fit$lambda.min 



y.predicted = predict(fit.enet, s = optimum.lambda, newx = x.train)

# Sum of Squares Total and Error for Training Data
sst = sum((y.train - mean(y.train))^2)
sse = sum((y.predicted - y.train)^2)

# R squared
rsq = 1 - sse / sst
rsq

# RMSE
rmse = sqrt(sse/nrow(train))
rmse


y.predicted.test = predict(fit.enet, s = optimum.lambda, newx = x.test)

# Sum of Squares Total and Error for Testing Data
sst = sum((y.test - mean(y.test))^2)
sse = sum((y.predicted.test - y.test)^2)

# R squared
rsq = 1 - sse / sst
cat("Rsquared Value for Test data = ", rsq, "\n")

# RMSE
rmse = sqrt(sse/nrow(train))
cat("RMSE Value for Test data = ", rmse)


plot(fit.enet, xvar = "lambda", label=T, xlim = c(-2, 5))
abline(v=cv.fit$lambda.min, col = "red", lty=2)
abline(v=cv.fit$lambda.1se, col="blue", lty=2)
title("Elastic Net Regression", line=2.5)
```

Observing the Coefficent VS Log Lambda plot, we can see that at lambda minimum there is great variation in the coefficient values. It can be visualized that 1,4 and 7 are great predictors for the target attribute. we can see that attribute 6 can be removed as it almost drops to zero near lambda minimum. Elastic net regression has a scope of further improvement as there are two parameters - alpha and lambda. This algorithm can be further fine tuned and is a good candidate for core algorithm. 

Alpha can be fine tuned and an optimum value can be found. This fine tuning would help improve the performance Elastic Net Regression. In order to fine tune alpha, the best or optimum alpha value should be calculated. Using cross validation, the optimum alpha can be figured out through using different values of alpha between 0.1 to 0.9.

```{r fineTuneAlpha, echo=FALSE}

#Function to calculate the best alpha using cross-validation
a = seq(0.1, 0.9, 0.05)
search = foreach(i = a, .combine = rbind) %dopar% {
  cv = cv.glmnet(x.train, y.train, nfold = 10, type.measure = "deviance", paralle = TRUE, alpha = i)
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.1se], lambda.1se = cv$lambda.1se, alpha = i)
}


cv3 = search[search$cvm == min(search$cvm), ]
alpha.optimum = cv3$alpha #optimum alpha 
```

After calculating the optimum alpha, we can use it to make a new fit with the optimized alpha value.This method is an upgrade to the core algorithm which is elastic net regression.

```{r optimizedElasticNet, echo=FALSE}

cat(alpha.optimum)
cat(optimum.lambda)

#Elastic Net Regression fit with optimum lambda and optimum alpha
optimized.elasticNet = glmnet(x.train, y.train, alpha = alpha.optimum, lambda= lambdas,  standardize = TRUE)
#coef(optimized.elasticNet) 

plot(optimized.elasticNet, xvar = "lambda", label=T, xlim = c(0, 5))

abline(v=optimum.lambda, col = "red", lty=2)
#abline(v=cv.fit$lambda.1se, col="red", lty=2)
title("Elastic Net Regression", line=2.5)

y.predicted = predict(optimized.elasticNet, s = optimum.lambda, newx = x.train)

# Sum of Squares Total and Error for Training Data
sst = sum((y.train - mean(y.train))^2)
sse = sum((y.predicted - y.train)^2)

# R squared
rsq = 1 - sse / sst
rsq

# RMSE
rmse = sqrt(sse/nrow(train))
rmse

y.predicted.test = predict(optimized.elasticNet, s = optimum.lambda, newx = x.test)

# Sum of Squares Total and Error for Testing Data
sst = sum((y.test - mean(y.test))^2)
sse = sum((y.predicted.test - y.test)^2)

# R squared
rsq = 1 - sse / sst
cat("Rsquared Value for Test data = ", rsq, "\n")

# RMSE
rmse.eNet = sqrt(sse/nrow(train))
cat("RMSE Value for Test data = ", rmse.eNet)

#coefpath(optimized.elasticNet)
```

# **eXtreme Gradient Boosting (XGB) Regression**

The eXtreme Gradient Boosting Regression is out of the course algortihm used in this project. Extreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more efficient. It has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to do parallel computation on a single machine. In comparison to random forest regression the xgboost is more better for the same reason that xgboost perform linear model solver and the tree learning algorithms.
The function intakes the training dataset and the labels.

```{r xgbRegression, echo=FALSE}
fit.xgb = xgboost(x.train, label = y.train, eta = 0.3, max_depth = 15, nrounds = 75, verbose = 0)

# Eta is the step size of each boosting step. 
# max_depth takes the maximum depth of the tree. 
# nrounds the max number of iterations. 
# Verbose is 0 that means it will print the rmse value for each iteration(nrounds).

y.predicted = predict(fit.xgb, x.train)

# Sum of Squares Total and Error for Training Data
sst = sum((y.train - mean(y.train))^2)
sse = sum((y.predicted - y.train)^2)

# R squared
rsq = 1 - sse / sst
rsq

# RMSE
rmse = sqrt(sse/nrow(train))
rmse

y.predicted.test = predict(fit.xgb, x.test)



# Sum of Squares Total and Error for Testing Data
sst = sum((y.test - mean(y.test))^2)
sse = sum((y.predicted.test - y.test)^2)

# R squared
rsq = 1 - sse / sst
cat("Rsquared Value for Test data = ", rsq, "\n")

# RMSE
rmse.xgb = sqrt(sse/nrow(train))
cat("RMSE Value for Test data = ", rmse.xgb)

```

# **Random Forest Regression**
Random Forest Regression uses ensemble classification for regression. It creates multiple decision tree and same time train them. The ouput of each tree is collected and then mode is chosen as the output of the Regression. Sometimes it is difficult to interpret the output of the random forest but it generally boosts the performance of the model.

```{r randomForest, echo=FALSE}
fit.rForest = randomForest(x.train, y.train, ntree = 150, replace = TRUE, maxnodes = 32)

# ntree is the number of tree.
# replace is for completely replacing the sample data set.
# maxnodes is used to control the depth of the tree.

y.predicted = predict(fit.rForest, x.train)

# Sum of Squares Total and Error for Training Data
sst = sum((y.train - mean(y.train))^2)
sse = sum((y.predicted - y.train)^2)

# R squared
rsq = 1 - sse / sst
rsq

# RMSE
rmse = sqrt(sse/nrow(train))
rmse

y.predicted.test = predict(fit.rForest, x.test)

# Sum of Squares Total and Error for Testing Data
sst = sum((y.test - mean(y.test))^2)
sse = sum((y.predicted.test - y.test)^2)

# R squared
rsq = 1 - sse / sst
cat("Rsquared Value for Test data = ", rsq, "\n")

# RMSE
rmse.rForest = sqrt(sse/nrow(train))
cat("RMSE Value for Test data = ", rmse.rForest)

```

# **Comparative Analysis and Conclusion**

For comparing the all the 5 models used we chose RMSE value. RMSE help us to compare model well as it is used to calculate the standard deviation of the errors and how far they are from the regression line for the model. 
The performaces of all models for test data are summarized below - 

```{r compareRMSE, echo=FALSE}
cat("1. RMSE value for Ridge Regression is ", rmse.ridge)
cat("2. RMSE value for Lasso Regression is ", rmse.lasso)
cat("3. RMSE value for Elastic Net Regression before tuning is ", rmse)
cat("4. RMSE value for Elastic Net Regression after tuning is ", rmse.eNet)
cat("5. RMSE value for XGBoost Regression is ", rmse.xgb)
cat("6. RMSE value for Random Forest Regression is ", rmse.rForest)

```

In conclusion, it can be observed that the eXtreme Gradient Boost Regression gives the least RMSE score and hence it performs the best. The reason for this is that the XGBoost uses both linear regression and tree algorithm. Another observation from the results for the black friday sales dataset is that the regression tree models perform better as compared to models using linear regression. RMSE for the elastic net regression improves after tuning both the parameters - alpha and lambda.
Most of the regression showed that 1, 4 and 7(Gender, City_Category and Product_Category_1) are the best predictors and the features that affect the target or predict the sales for the black friday dataset the most.